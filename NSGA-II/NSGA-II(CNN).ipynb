{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e846780-db6b-45e5-9b59-bfa0d6b7049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from functools import partial\n",
    "from math import exp\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only the first GPU as visible\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Allow memory growth to allocate memory dynamically on the GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU configuration successful.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b25348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.mixed_precision import Policy\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    X_train = np.load(f'{data_path}/X_train.npy')\n",
    "    X_test = np.load(f'{data_path}/X_test.npy')\n",
    "    X_validation = np.load(f'{data_path}/X_val.npy')\n",
    "\n",
    "    y_train = np.load(f'{data_path}/y_train.npy')\n",
    "    y_test = np.load(f'{data_path}/y_test.npy')\n",
    "    y_validation = np.load(f'{data_path}/y_val.npy')\n",
    "\n",
    "    y_train = y_train[..., np.newaxis]\n",
    "    y_test = y_test[..., np.newaxis]\n",
    "    y_validation = y_validation[..., np.newaxis]\n",
    "\n",
    "    print(\"Dataset loaded!\")\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, X_validation, y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51021617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path):\n",
    "    \"\"\"Creates train, validation and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # load dataset\n",
    "    X_train, X_test, X_validation, y_train, y_test, y_validation = load_data(data_path)\n",
    "    \n",
    "################# Scaleing the data ####################\n",
    "    scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = X_train.shape\n",
    "    X_train = X_train.reshape(-1, num_features)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_train = X_train.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_test.shape\n",
    "    X_test = X_test.reshape(-1, num_features)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_test = X_test.reshape(num_instances, num_time_steps, num_features) \n",
    "\n",
    "    num_instances, num_time_steps, num_features = X_validation.shape\n",
    "\n",
    "    X_validation = X_validation.reshape(-1, num_features)\n",
    "    X_validation = scaler.fit_transform(X_validation)\n",
    "    \n",
    "     #reshapeing\n",
    "    X_validation = X_validation.reshape(num_instances, num_time_steps, num_features)  \n",
    "    \n",
    "    # Save the scaler to a file\n",
    "    joblib.dump(scaler, './scaler/scaler.pkl')\n",
    "\n",
    "    # add an axis to nd array\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ec.gpu/Desktop/Soumen/kws/data_npy\"\n",
    "class_names = ['off', 'left', 'down', 'up', 'go', 'on', 'stop', 'unknown', 'right', 'yes']  #, 'silence' , 'no'\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "SKIP = 1\n",
    "CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train, validation and test sets\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078edd8b",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f292c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "import random\n",
    "\n",
    "results_1 = []\n",
    "\n",
    "# Results to store specifications of models and their fitness values per generation\n",
    "generation_pareto_fronts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom crossover\n",
    "def cxUniform(ind1, ind2, indpb):\n",
    "    for i in range(len(ind1)):\n",
    "        if random.random() < indpb:\n",
    "            ind1[i], ind2[i] = ind2[i], ind1[i]\n",
    "    return ind1, ind2\n",
    "\n",
    "# Define custom mutation\n",
    "def mutFlipBit(individual, indpb):\n",
    "    for i in range(len(individual)):\n",
    "        if random.random() < indpb:\n",
    "            if isinstance(individual[i], bool):\n",
    "                individual[i] = not individual[i]\n",
    "            elif isinstance(individual[i], list):\n",
    "                individual[i] = [random.choice([128, 256]) if isinstance(x, int) else x for x in individual[i]]\n",
    "            elif isinstance(individual[i], tuple):\n",
    "                individual[i] = (random.choice([3, 5]), random.choice([3, 5]))\n",
    "            else:\n",
    "                individual[i] = random.choice([16, 32, 64])\n",
    "    return individual,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN model \n",
    "def create_2d_cnn_model(conv_layers, filters, input_shape, kernel_size, fc_layers, use_bn, use_dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    if use_bn:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    for _ in range(conv_layers - 1):\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        if use_bn:\n",
    "            model.add(BatchNormalization())\n",
    "        current_shape = model.output_shape\n",
    "        if current_shape[1] > 2 and current_shape[2] > 2:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    for neurons in fc_layers:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(CLASS, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='c', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301aab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, epochs, batch_size, patience, X_train, y_train, X_validation, y_validation):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation),\n",
    "                        epochs=epochs, batch_size=batch_size, callbacks=[early_stopping], verbose=0)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987af883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and return accuracy and size\n",
    "def evaluate_model(conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout):\n",
    "   \n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], 1) \n",
    "    model = create_2d_cnn_model(conv_layers, filters, input_shape, kernel_size, fc_layers, use_bn, use_dropout)\n",
    "    history = train(model, EPOCHS, BATCH_SIZE, PATIENCE, X_train, y_train, X_validation, y_validation)\n",
    "\n",
    "    val_accuracy = model.evaluate(X_validation, y_validation, verbose=0)[1]\n",
    "    model_size = model.count_params()\n",
    "    results_1.append((conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout, val_accuracy, model_size))\n",
    "    # Print the model summary\n",
    "    # print(f\"Layers: {conv_layers}, Filters: {filters}, Kernel: {kernel_size}, FC: {fc_layers} , BN: {use_bn}, Dropout: {use_dropout} - Val_Acc: {val_accuracy:.4f}, Params: {model_size}\")\n",
    "    #plot_history(history)\n",
    "    \n",
    "    return 1 - val_accuracy, model_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(individual):\n",
    "    conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout = individual\n",
    "    return evaluate_model(conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ee47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSGA-II setup\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, 1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_conv_layers\", np.random.randint, 1, 6)\n",
    "toolbox.register(\"attr_filters\", np.random.choice, [16, 32, 64])\n",
    "toolbox.register(\"attr_kernel_size\", lambda: (np.random.choice([3, 5]), np.random.choice([3, 5])))\n",
    "toolbox.register(\"attr_fc_layers\", lambda: [np.random.choice([128, 256, 512])])\n",
    "toolbox.register(\"attr_use_bn\", np.random.choice, [True, False])\n",
    "toolbox.register(\"attr_use_dropout\", np.random.choice, [True, False])\n",
    "\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_conv_layers, toolbox.attr_filters, toolbox.attr_kernel_size,\n",
    "                  toolbox.attr_fc_layers, toolbox.attr_use_bn, toolbox.attr_use_dropout),\n",
    "                 n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", cxUniform, indpb=0.5)\n",
    "toolbox.register(\"mutate\", mutFlipBit, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "toolbox.register(\"map\", map)\n",
    "\n",
    "# GA parameters\n",
    "population_size = 30\n",
    "num_generations =10\n",
    "crossover_prob = 0.5\n",
    "mutation_prob = 0.2\n",
    "\n",
    "# Generate the population and run the algorithm\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Capture Pareto fronts for each generation\n",
    "generation_pareto_fronts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eff009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to capture Pareto front and model specifications at each generation\n",
    "def capture_generation_pareto(population, gen_number, results_1):\n",
    "    \n",
    "    # Initialize costs as a list\n",
    "    costs = []\n",
    "    for result in results_1:\n",
    "        conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout, val_accuracy, model_size = result\n",
    "        costs.append((1 - val_accuracy, model_size))\n",
    "    \n",
    "    costs = np.array(costs)\n",
    "\n",
    "    # Find Pareto-optimal models\n",
    "    pareto_indices = is_pareto_efficient(costs)\n",
    "\n",
    "    # Print Pareto-optimal results\n",
    "    print(\"Pareto-optimal models:\")\n",
    "    for idx in pareto_indices:\n",
    "        print(f\"Configuration: {results_1[idx]}\")\n",
    "        \n",
    "    # Convert results to DataFrame\n",
    "    columns = ['Generation', 'Conv_Layers', 'Filters', 'Kernel_Size', 'FC_Layers', 'Use_BN', 'Use_Dropout', 'Validation_Accuracy', 'Model_Size']\n",
    "\n",
    "    # DataFrame for all models\n",
    "    df_all_models = pd.DataFrame(results_1, columns=columns[1:])\n",
    "    df_all_models.insert(0, 'Generation', gen_number)\n",
    "\n",
    "    # DataFrame for Pareto models\n",
    "    pareto_models = [results_1[idx] for idx in pareto_indices]\n",
    "    df_pareto = pd.DataFrame(pareto_models, columns=columns[1:])\n",
    "    df_pareto.insert(0, 'Generation', gen_number)\n",
    "\n",
    "    # Append to CSV files (create if they don't exist)\n",
    "    df_all_models.to_csv('CNN/all_models.csv', mode='a', header=not pd.io.common.file_exists('CNN/all_models.csv'), index=False)\n",
    "    df_pareto.to_csv('CNN/pareto_front.csv', mode='a', header=not pd.io.common.file_exists('CNN/pareto_front.csv'), index=False)\n",
    "    \n",
    "\t\t\t\t\n",
    "\t\t\t\t# Plot Pareto frontier\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Scatter plot for all models\n",
    "    for i, result in enumerate(results_1):\n",
    "        conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout, val_accuracy, model_size = result\n",
    "        label = (f'Model {i+1}: {conv_layers} layers, {filters} filters, kernel {kernel_size}, '\n",
    "                 f'fc {fc_layers}, BN: {use_bn}, Dropout: {use_dropout}')\n",
    "        plt.scatter(model_size, val_accuracy, label=label, s=100)\n",
    "        if i in pareto_indices:\n",
    "            plt.annotate(f'{i+1}', (model_size, val_accuracy), textcoords=\"offset points\", xytext=(0, 10), ha='center', color='red')\n",
    "\n",
    "    # Extract Pareto frontier points\n",
    "    pareto_points = costs[pareto_indices]\n",
    "\n",
    "    # Plot Pareto frontier line\n",
    "    if len(pareto_points) > 0:\n",
    "        plt.plot(pareto_points[:, 1], 1 - pareto_points[:, 0], color='red', linestyle='--', linewidth=2, label='Pareto Frontier')\n",
    "\n",
    "    plt.xlabel('Model Size (Number of Parameters)')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title('Pareto Frontier: Model Size vs. Validation Accuracy')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a2ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check Pareto efficiency\n",
    "def is_pareto_efficient(costs):\n",
    "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n",
    "            is_efficient[i] = True\n",
    "    return np.where(is_efficient)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for running generations\n",
    "for gen in range(num_generations):\n",
    "    print(f\"######## Generation: { gen } ###########\")\n",
    "    # Apply selection, crossover, and mutation\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = algorithms.varAnd(offspring, toolbox, cxpb=crossover_prob, mutpb=mutation_prob)\n",
    "    \n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # Select the next generation population\n",
    "    population[:] = toolbox.select(population + offspring, population_size)\n",
    "    \n",
    "    \n",
    "    # Capture Pareto front for the current generation\n",
    "    capture_generation_pareto(population, gen, results_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
