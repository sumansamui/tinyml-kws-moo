{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from functools import partial\n",
    "from math import exp\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only the first GPU as visible\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Allow memory growth to allocate memory dynamically on the GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU configuration successful.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.mixed_precision import Policy\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset.\n",
    "    \"\"\"\n",
    "    X_train = np.load(f'{data_path}/X_train.npy')\n",
    "    X_test = np.load(f'{data_path}/X_test.npy')\n",
    "    X_validation = np.load(f'{data_path}/X_val.npy')\n",
    " \n",
    "    y_train = np.load(f'{data_path}/y_train.npy')\n",
    "    y_test = np.load(f'{data_path}/y_test.npy')\n",
    "    y_validation = np.load(f'{data_path}/y_val.npy')\n",
    "\n",
    "    y_train = y_train[..., np.newaxis]\n",
    "    y_test = y_test[..., np.newaxis]\n",
    "    y_validation = y_validation[..., np.newaxis]\n",
    "\n",
    "    print(\"Dataset loaded!\")\n",
    "\n",
    "    return X_train, X_test, X_validation, y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path):\n",
    "    \"\"\"Creates train, validation and test sets.\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    X_train, X_test, X_validation, y_train, y_test, y_validation = load_data(data_path)\n",
    "    \n",
    "    ########## Scaleing the data ##########\n",
    "    scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = X_train.shape\n",
    "    X_train = X_train.reshape(-1, num_features)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_train = X_train.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_test.shape\n",
    "    X_test = X_test.reshape(-1, num_features)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_test = X_test.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_validation.shape\n",
    "    X_validation = X_validation.reshape(-1, num_features)\n",
    "    X_validation = scaler.fit_transform(X_validation)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_validation = X_validation.reshape(num_instances, num_time_steps, num_features) \n",
    "    \n",
    "    # Save the scaler to a file\n",
    "    joblib.dump(scaler, './scaler/scaler.pkl')\n",
    "    \n",
    "    # add an axis to nd array\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ec.gpu/Desktop/Soumen/Dataset/kws/data_npy\"\n",
    "class_names = ['off', 'left', 'down', 'up', 'go', 'on', 'stop', 'unknown', 'right', 'yes']  #, 'silence' , 'no'\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16  #64\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "SKIP = 1\n",
    "CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train, validation and test sets\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, LSTM , Reshape, Lambda\n",
    "\n",
    "def create_CRNN_model(conv_layers,lstm_layers,  filters, kernel_size, fc_layers, use_bn, use_dropout):\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], 1) \n",
    "    model = Sequential()\n",
    "    for _ in range(conv_layers):\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape, padding='same'))\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        if use_bn:\n",
    "            model.add(BatchNormalization())\n",
    "        current_shape = model.output_shape\n",
    "        if current_shape[1] > 2 and current_shape[2] > 2:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Lambda(lambda x: tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2] * tf.shape(x)[3]))))\n",
    "    for _ in range(lstm_layers):\n",
    "                            model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Flatten())      \n",
    "    for neurons in fc_layers:\n",
    "        if neurons == 4:\n",
    "                model.add(Dense(512, activation='relu'))\n",
    "        if neurons == 3:\n",
    "            model.add(Dense(256, activation='relu'))\n",
    "        if neurons == 2:\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "        if neurons == 1:\n",
    "            model.add(Dense(64, activation='relu'))\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(CLASS, activation='softmax'))\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model):\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation),\n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # Return validation accuracy (simulate it) and model size\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    num_params = model.count_params()\n",
    "    return accuracy, num_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization Setup Using Gaussian Processes (GPflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "from gpflow.utilities import print_summary\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# Objective function to optimize\n",
    "def objective_function(conv_layers,lstm_layers, filters, kernel_size, fc_layers, use_bn, use_dropout):\n",
    "    filters = max(int(filters), 8) \n",
    "    use_bn = bool(round(use_bn))\n",
    "    use_dropout = bool(round(use_dropout))\n",
    "        # Ensure fc_layers is a list\n",
    "    if isinstance(fc_layers, int):\n",
    "        fc_layers = [fc_layers]\n",
    "    model = create_CRNN_model(conv_layers,lstm_layers,  filters, kernel_size, fc_layers, use_bn, use_dropout)\n",
    "    acc, params = train_evaluate_model(model)\n",
    "    \n",
    "    return np.array([acc, -params])  \n",
    "\n",
    "# Define initial hyperparameters\n",
    "conv_layers = [ 1, 2, 3]\t\t\t\n",
    "lstm_layers = [ 1, 2]  \n",
    "filters = [16, 32, 64]\n",
    "kernel_size = [(3, 3), (5, 5)]\n",
    "fc_layers = [1, 2, 3]  \n",
    "use_bn = [1, 0]  \t\t\t\t\t\t\t# Representing True as 1 and False as 0\n",
    "use_dropout = [1, 0]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_space = [\n",
    "    {'conv_layers': cl, 'lstm_layers': ll,'filters': f, 'kernel_size': ks, 'fc_layers': fc, 'use_bn': bn, 'use_dropout': do}\n",
    "    for cl, ll, f, ks, fc, bn, do in itertools.product(conv_layers,lstm_layers, filters, kernel_size, fc_layers, use_bn, use_dropout)\n",
    "]\n",
    "\n",
    "# Collect initial data\n",
    "X_init = np.array([[p['conv_layers'],p['lstm_layers'], p['filters'], p['kernel_size'][0],p['kernel_size'][1], p['fc_layers'], p['use_bn'], p['use_dropout']] for p in param_space])\n",
    "Y_init = np.array([objective_function(**p) for p in param_space])\n",
    "\n",
    "# Verify data shapes\n",
    "print(\"X_init shape:\", X_init.shape)\n",
    "print(\"Y_init shape:\", Y_init.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_init)\n",
    "\n",
    "# Create Gaussian Process models for each objective\n",
    "kern_acc = gpflow.kernels.Matern52()\n",
    "kern_size = gpflow.kernels.Matern52()\n",
    "\n",
    "gp_acc = gpflow.models.GPR(data=(X_scaled, Y_init[:, 0:1]), kernel=kern_acc)\n",
    "gp_size = gpflow.models.GPR(data=(X_scaled, Y_init[:, 1:2]), kernel=kern_size)\n",
    "\n",
    "# Optimize GP hyperparameters\n",
    "gpflow.optimizers.Scipy().minimize(gp_acc.training_loss, gp_acc.trainable_variables)\n",
    "gpflow.optimizers.Scipy().minimize(gp_size.training_loss, gp_size.trainable_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store hyperparameters, accuracy, and model size\n",
    "pareto_data = []\n",
    "\n",
    "# Function to save the collected Pareto front data to CSV\n",
    "def save_pareto_data_to_csv(pareto_data, filename=\"DS-CNN_pareto_front.csv\"):\n",
    "    \n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    df = pd.DataFrame(pareto_data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, mode='a', index=False)\n",
    "\n",
    "# Acquisition function \n",
    "def acquisition_function(x):\n",
    "    \n",
    "    # Reshape x to be 2D [1, D] before passing to GP models\n",
    "    x_reshaped = x.reshape(1, -1)  # Shape should be [1, D]\n",
    "    \n",
    "    # Predict mean and variance for each GP model\n",
    "    mu_acc, var_acc = gp_acc.predict_f(x_reshaped)\n",
    "    mu_size, var_size = gp_size.predict_f(x_reshaped)\n",
    "    \n",
    "    # Calculate acquisition score based on objectives (simplified in this case)\n",
    "    return - (mu_acc + mu_size)  \n",
    "\n",
    "# Define your objective function, which evaluates the model using the given hyperparameters\n",
    "def objective_function_1(hyperparameters):\n",
    "\tconv_layers = hyperparameters['conv_layers']\n",
    "\tlstm_layers = hyperparameters['lstm_layers']\n",
    "\tfilters = hyperparameters['filters']\n",
    "\tkernel_size = hyperparameters['kernel_size']\n",
    "\tfc_layers = hyperparameters['fc_layers']\n",
    "\tuse_bn = hyperparameters['use_bn']\n",
    "\tuse_dropout = hyperparameters['use_dropout']\n",
    "\tif isinstance(fc_layers, int):\n",
    "\t\tfc_layers = [fc_layers]\n",
    "\tmodel = create_CRNN_model(conv_layers,lstm_layers, filters, kernel_size, fc_layers, use_bn, use_dropout)\n",
    "\taccuracy, model_size = train_evaluate_model(model)  \n",
    "\treturn np.array([accuracy, model_size])  # Must match the format of Y_scaled\n",
    "\n",
    "# Function to map scaled values to the original hyperparameter values\n",
    "def map_to_hyperparameters(new_x, param_space):\n",
    "    hyperparameters = {}\n",
    "\n",
    "    # Rescale the index of each parameter based on the scaled value\n",
    "    for i, param in enumerate(param_space[0].keys()):\n",
    "        \n",
    "        # Get the values for the current parameter across all configurations\n",
    "        param_values = [x[param] for x in param_space]\n",
    "        \n",
    "        # Clamp the value between 0 and 1 to avoid out-of-range errors\n",
    "        clamped_value = min(max(new_x[0, i], 0), 1)\n",
    "        \n",
    "        # Map the clamped value to an index in the param_values list\n",
    "        idx = int(clamped_value * (len(param_values) - 1))\n",
    "        \n",
    "        # Assign the parameter value to the hyperparameters dictionary\n",
    "        hyperparameters[param] = param_values[idx]\n",
    "\n",
    "    return hyperparameters\n",
    "\n",
    "# Bayesian optimization loop\n",
    "n_iterations = 100  # Number of iterations for optimization\n",
    "for i in range(n_iterations):\n",
    "    \n",
    "    # Use a random point from X_scaled as the starting point\n",
    "    x0 = X_scaled[np.random.choice(X_scaled.shape[0]), :].flatten()  # Flatten to make it 1D array\n",
    "    \n",
    "    # Minimize the acquisition function\n",
    "    result = minimize(acquisition_function, x0, method='L-BFGS-B')\n",
    "    \n",
    "    # Get the new point from the optimization result\n",
    "    new_x = result.x.reshape(1, -1)  # Ensure new_x is 2D\n",
    "    \n",
    "    # Map the optimized values to the real hyperparameter space\n",
    "    hyperparameters = map_to_hyperparameters(new_x, param_space)  # Use new_x directly, not result.x\n",
    "    print(f\"Iteration {i+1}: Hyperparameters = {hyperparameters}\")\n",
    "\n",
    "    # Evaluate the new point using the objective function\n",
    "    new_y = objective_function_1(hyperparameters).reshape(1, -1)\n",
    "    \n",
    "    # Update GPs with new data\n",
    "    X_scaled = np.vstack((X_scaled, new_x))\n",
    "    Y_scaled = np.vstack((Y_init, new_y))\n",
    "    X_scaled = X_scaled[-len(Y_scaled):]    # Keep both arrays the same length\n",
    "\n",
    "    pareto_data.append({\n",
    "        \"iterations\": i,\n",
    "        \"conv_layers\": hyperparameters['conv_layers'],\n",
    "        \"lstm_layers\": hyperparameters['lstm_layers'],\n",
    "        \"filters\": hyperparameters['filters'],\n",
    "        \"kernel_size\": hyperparameters['kernel_size'],\n",
    "        \"fc_layers\": hyperparameters['fc_layers'],\n",
    "        \"use_bn\": hyperparameters['use_bn'],\n",
    "        \"use_dropout\": hyperparameters['use_dropout'],\n",
    "        \"accuracy\": new_y[0, 0],  # Accuracy value\n",
    "        \"model_size\": new_y[0, 1]  # Model size value\n",
    "    })\n",
    "    \n",
    "    # Recreate and re-optimize the GP models with updated data\n",
    "    gp_acc = gpflow.models.GPR(data=(X_scaled, Y_scaled[:, 0:1]), kernel=kern_acc)\n",
    "    gp_size = gpflow.models.GPR(data=(X_scaled, Y_scaled[:, 1:2]), kernel=kern_size)\n",
    "    gpflow.optimizers.Scipy().minimize(gp_acc.training_loss, gp_acc.trainable_variables)\n",
    "    gpflow.optimizers.Scipy().minimize(gp_size.training_loss, gp_size.trainable_variables)\n",
    "\n",
    "# Save the collected Pareto data to a CSV file after the loop finishes\n",
    "save_pareto_data_to_csv(pareto_data, \"pareto_front.csv\")\n",
    "\n",
    "# Plot Pareto front\n",
    "\n",
    "plt.scatter(Y_scaled[:, 1], Y_scaled[:, 0], color='red') \n",
    "plt.xlabel('Model Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Pareto Front')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def is_dominated(candidate, front):\n",
    "    \"\"\"\n",
    "    Check if the candidate solution is dominated by any solution in the Pareto front.\n",
    "    candidate: The new candidate solution (accuracy, model_size).\n",
    "    front: List of Pareto-optimal solutions so far.\n",
    "    \"\"\"\n",
    "    for solution in front:\n",
    "        \n",
    "        # A solution 'solution' dominates 'candidate' if it is better in all objectives\n",
    "        if all(c <= s for c, s in zip(candidate, solution)) and any(c < s for c, s in zip(candidate, solution)):\n",
    "            return True  # candidate is dominated by solution\n",
    "    return False  # candidate is not dominated\n",
    "\n",
    "def get_pareto_front_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Extract the Pareto front from the CSV file containing all the solutions.\n",
    "    csv_file: The path to the CSV file containing the solutions.\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract accuracy and model size columns (assuming these columns exist)\n",
    "    pareto_front = []\n",
    "    for _, row in data.iterrows():\n",
    "        candidate = [row['accuracy'], row['model_size']]\n",
    "        \n",
    "        # Update the Pareto front if the candidate is not dominated\n",
    "        if not is_dominated(candidate, pareto_front):\n",
    "            \n",
    "            # Remove solutions dominated by the candidate\n",
    "            pareto_front = [s for s in pareto_front if not is_dominated(s, [candidate])]\n",
    "            \n",
    "            # Add the candidate as a Pareto-optimal solution\n",
    "            pareto_front.append(candidate)\n",
    "\n",
    "    return pareto_front\n",
    "\n",
    "# Path to the CSV file containing all the solutions\n",
    "csv_file = \"pareto_front.csv\"  # Change this to your actual file path\n",
    "\n",
    "# Get the Pareto front from the CSV data\n",
    "pareto_front = get_pareto_front_from_csv(csv_file)\n",
    "\n",
    "# Save the Pareto front to a new CSV file\n",
    "pareto_front_df = pd.DataFrame(pareto_front, columns=['accuracy', 'model_size'])\n",
    "pareto_front_df.to_csv('pareto_front_extracted.csv', index=False)\n",
    "\n",
    "# Optionally, visualize the Pareto front\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pareto_front_df['model_size'], pareto_front_df['accuracy'], color='red')\n",
    "plt.xlabel('Model Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Pareto Front')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_objective",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
