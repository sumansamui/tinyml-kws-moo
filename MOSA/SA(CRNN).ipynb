{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from functools import partial\n",
    "from math import exp\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from itertools import cycle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only the first GPU as visible\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Allow memory growth to allocate memory dynamically on the GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU configuration successful.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.mixed_precision import Policy\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset.\n",
    "    \"\"\"\n",
    "    \t\n",
    "    X_train = np.load(f'{data_path}/X_train.npy')\n",
    "    X_test = np.load(f'{data_path}/X_test.npy')\n",
    "    X_validation = np.load(f'{data_path}/X_val.npy')\n",
    "\n",
    "    y_train = np.load(f'{data_path}/y_train.npy')\n",
    "    y_test = np.load(f'{data_path}/y_test.npy')\n",
    "    y_validation = np.load(f'{data_path}/y_val.npy')\n",
    "\n",
    "    y_train = y_train[..., np.newaxis]\n",
    "    y_test = y_test[..., np.newaxis]\n",
    "    y_validation = y_validation[..., np.newaxis]\n",
    "    print(\"Dataset loaded!\")\n",
    "\n",
    "    return X_train, X_test, X_validation, y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path):\n",
    "    \"\"\"Creates train, validation and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load dataset\n",
    "    X_train, X_test, X_validation, y_train, y_test, y_validation = load_data(data_path)\n",
    "    \n",
    "\t\t\t\t######## Scaleing the data ########\n",
    "    scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = X_train.shape\n",
    "    X_train = X_train.reshape(-1, num_features)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_train = X_train.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_test.shape\n",
    "    X_test = X_test.reshape(-1, num_features)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    #reshapeing\n",
    "    X_test = X_test.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_validation.shape\n",
    "    X_validation = X_validation.reshape(-1, num_features\n",
    "    X_validation = scaler.fit_transform(X_validation)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_validation = X_validation.reshape(num_instances, num_time_steps, num_features) \n",
    "    \n",
    "    # Save the scaler to a file\n",
    "    joblib.dump(scaler, './scaler/scaler.pkl')\n",
    "\n",
    "    # add an axis to nd array\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ec.gpu/Desktop/Soumen/Dataset/kws/data_npy\"    \n",
    "class_names = ['off', 'left', 'down', 'up', 'go', 'on', 'stop', 'unknown', 'right', 'yes']  #, 'silence' , 'no'\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 5 \n",
    "LEARNING_RATE = 0.0001\n",
    "SKIP = 1\n",
    "CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train, validation and test sets\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LSTM , Reshape, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Objective functions #####################\n",
    "def objective_accuracy(model, X_val, y_val):\n",
    "    \"\"\"Compute validation accuracy.\"\"\"\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def objective_model_size(model):\n",
    "    \"\"\"Compute model size in megabytes.\"\"\"\n",
    "    \n",
    "    # model_size = sum([np.prod(v.get_shape().as_list()) for v in model.trainable_weights]) #* 4 / (1024 ** 2)\n",
    "    model_size = sum([np.prod(v.shape.as_list()) for v in model.trainable_weights])\n",
    "    # model_size = model.count_params()\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a neighboring solution\n",
    "import random\n",
    "\n",
    "def perturb_solution(solution, bounds):\n",
    "    \"\"\"\n",
    "    Generate a perturbed solution using a concise approach within the given bounds.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Conv layers (integer range)\n",
    "        max(bounds[0][0], min(bounds[0][1], solution[0] + random.randint(-1, 1))),\n",
    "\n",
    "        # Conv layers (integer range)\n",
    "        max(bounds[1][0], min(bounds[1][1], solution[1] + random.randint(-1, 1))),\n",
    "        \n",
    "        # Filters (integer range)\n",
    "        max(bounds[2][0], min(bounds[2][1], solution[2] + random.randint(-32, 32))),\n",
    "        \n",
    "        # Kernel size (choice from list)\n",
    "        random.choice(bounds[3]),\n",
    "        \n",
    "        # FC layers (list of integers with random perturbations)\n",
    "        [max(bounds[5][0], min(bounds[5][1], fc + random.randint(-8, 8)))\n",
    "         for fc in solution[4]],\n",
    "        \n",
    "        # Batch Normalization (choice from list)\n",
    "        random.choice(bounds[6]),\n",
    "        \n",
    "        # Dropout (choice from list)\n",
    "        random.choice(bounds[7])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Create a CRNN model \n",
    "def create_CRNN_model(conv_layers,lstm_layers,  filters, kernel_size, fc_layers, use_bn, use_dropout, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape)) \n",
    "    for _ in range(conv_layers):\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        if use_bn:\n",
    "            model.add(BatchNormalization())\n",
    "        current_shape = model.output_shape\n",
    "        if current_shape[1] > 2 and current_shape[2] > 2:\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(Lambda(lambda x: tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2] * tf.shape(x)[3]))))\n",
    "    for _ in range(lstm_layers):\n",
    "                            model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Flatten())      \n",
    "    for neurons in fc_layers:\n",
    "        if neurons == 4:\n",
    "                model.add(Dense(512, activation='relu'))\n",
    "        if neurons == 3:\n",
    "            model.add(Dense(256, activation='relu'))\n",
    "        if neurons == 2:\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "        if neurons == 1:\n",
    "            model.add(Dense(64, activation='relu'))\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(CLASS, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Simulated Annealing with Multi-Objective Optimization #####################\n",
    "#################################################################################################\n",
    "def chaos_simulated_annealing(bounds, max_iter, cooling_rate=0.012, initial_temps=(10, 10), input_shape=input_shape, T_min=1e-6, no_improve_iter=20):\n",
    "    \n",
    "    # Initialize solution and archive\n",
    "    current_solution = [\n",
    "                        random.randint(*bounds[0]),                    # Conv layers\n",
    "                        random.randint(*bounds[1]),                    # LSTM layers\n",
    "                        random.randint(*bounds[2]),                    # Filters\n",
    "                        random.choice(bounds[3]),                      # Kernel size\n",
    "                        [random.randint(*bounds[5]) for _ in range(random.randint(*bounds[4]))],  # FC layers\n",
    "                        random.choice(bounds[6]),                      # Batch Normalization\n",
    "                        random.choice(bounds[7])                       # Dropout\n",
    "                        ]\n",
    "    temperatures = list(initial_temps)\n",
    "    archive = []\n",
    "    archive_all = []  # Initialize archive_all\n",
    "    no_improve_count = 0  # Counter for no improvement\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        print(f\"Iteration {iteration+1}/{max_iter}\")\n",
    "    \n",
    "\t\t\t\t\t\t\t\t# Check if temperatures have reached the threshold\n",
    "        if all(t < T_min for t in temperatures):\n",
    "            print(\"Temperatures are below the minimum threshold. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Generate a neighbor solution\n",
    "        new_solution = perturb_solution(current_solution, bounds)\n",
    "        \n",
    "        # Build and evaluate models for current and new solutions\n",
    "        current_model = create_CRNN_model(*current_solution, input_shape=input_shape)\n",
    "        new_model = create_CRNN_model(*new_solution, input_shape=input_shape)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=PATIENCE)\n",
    "        current_model.fit(X_train, y_train, epochs=500, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "        new_model.fit(X_train, y_train, epochs=500, batch_size=32, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        current_obj = [\n",
    "            objective_accuracy(current_model, X_validation, y_validation),\n",
    "            objective_model_size(current_model)\n",
    "        ]\n",
    "        new_obj = [\n",
    "            objective_accuracy(new_model, X_validation, y_validation),\n",
    "            objective_model_size(new_model)\n",
    "        ]\n",
    "        \n",
    "        print(f\"current_obj: {current_obj}\")\n",
    "        print(f\"new_obj: {new_obj}\")\n",
    "        \n",
    "        # Acceptance probability for each objective\n",
    "        accept_probs = [\n",
    "                        # For the first objective (accuracy), which should increase\n",
    "                        1 if new_obj[0] > current_obj[0] else \n",
    "                        np.exp(-(new_obj[0] - current_obj[0]) / temperatures[0]),  \n",
    "\n",
    "                        # For the second objective (model size), which should decrease\n",
    "                        1 if new_obj[1] < current_obj[1] else \n",
    "                        np.exp(-(new_obj[1] - current_obj[1]) / temperatures[1]) \n",
    "                        ]\n",
    "        \n",
    "        # Check acceptance\n",
    "        if any(p == 1 for p in accept_probs):\n",
    "            current_solution = new_solution\n",
    "            archive_all.append((current_solution, new_obj))  # Store all evaluated good solutions\n",
    "            print(\"archive_all\")\n",
    "            \n",
    "        else:\n",
    "            all_accepted = True\n",
    "            for p in accept_probs:\n",
    "                if random.random() >= p:\n",
    "                    all_accepted = False\n",
    "                    break\n",
    "            if all_accepted:\n",
    "                current_solution = new_solution\n",
    "                archive_all.append((current_solution, new_obj))  # Store all evaluated good solutions\n",
    "                print(\"archive_all\")\n",
    "        print(f\"accept_probs: {accept_probs}\")\n",
    "        temperatures = [t * math.exp(-cooling_rate) for t in temperatures]\n",
    "        print(f\"temperatures: {temperatures}\")      \n",
    "        print(\"###############################################\")\n",
    "    return archive_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Define bounds for hyperparameters #####################\n",
    "bounds = [\n",
    "    (1, 3),                     # Number of convolutional layers\n",
    "    (1,3),                      # Number of LSTM layers\n",
    "    (16, 64),                   # Number of filters\n",
    "    [(3, 3), (5, 5)],           # Kernel size\n",
    "    (1, 4),                     # Number of fully connected layers (1 to 4 layers)\n",
    "    (128, 512),                  # Number of neurons per fully connected layer\n",
    "    [True, False],              # Batch Normalization\n",
    "    [True, False]               # Dropout\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################### Run CSA #####################\n",
    "archive_all = chaos_simulated_annealing(bounds, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"All Archive:\", archive_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "def store_pareto_parameter_archive_all(archive, filename=\"CRNN_all__archive.xlsx\"):\n",
    "    \"\"\"\n",
    "    Store the final Pareto archive in an Excel file.\n",
    "    \"\"\"\n",
    "    # Prepare data for saving\n",
    "    data = []\n",
    "    for idx, (solution, objectives) in enumerate(archive):\n",
    "        # Flatten the solution and append objectives\n",
    "        record = {\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Solution ID\": f\"Solution_{idx + 1}\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Conv Layers\": solution[0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"LSTM Layers\": solution[1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Filters\": solution[2],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Kernel Size\": solution[3],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"FC Layers\": \"-\".join(map(str, solution[4])),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Batch Normalization\": solution[5],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"Dropout\": solution[6],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"accuracy\": objectives[0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"model_size\": objectives[1]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t}\n",
    "        data.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Pareto archive saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pareto_parameter_archive_all(archive_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
