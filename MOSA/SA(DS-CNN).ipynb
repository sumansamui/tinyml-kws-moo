{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import csv\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from functools import partial\n",
    "from math import exp\n",
    "from keras.utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report \n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from itertools import cycle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b17464",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only the first GPU as visible\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Allow memory growth to allocate memory dynamically on the GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU configuration successful.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.mixed_precision import Policy\n",
    "from keras.mixed_precision import set_global_policy\n",
    "\n",
    "policy = Policy('mixed_float16')\n",
    "set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads training dataset.\n",
    "    \"\"\"\n",
    "    X_train = np.load(f'{data_path}/X_train.npy')\n",
    "    X_test = np.load(f'{data_path}/X_test.npy')\n",
    "    X_validation = np.load(f'{data_path}/X_val.npy')\n",
    "    \n",
    "    y_train = np.load(f'{data_path}/y_train.npy')\n",
    "    y_test = np.load(f'{data_path}/y_test.npy')\n",
    "    y_validation = np.load(f'{data_path}/y_val.npy')\n",
    "\n",
    "    y_train = y_train[..., np.newaxis]\n",
    "    y_test = y_test[..., np.newaxis]\n",
    "    y_validation = y_validation[..., np.newaxis]\n",
    "\n",
    "    print(\"Dataset loaded!\")\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, X_validation, y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1559d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path):\n",
    "    \"\"\"Creates train, validation and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # load dataset\n",
    "    X_train, X_test, X_validation, y_train, y_test, y_validation = load_data(data_path)\n",
    "\n",
    "    ############ Scaleing the data ###########\n",
    "    scaler = StandardScaler()\n",
    "    num_instances, num_time_steps, num_features = X_train.shape\n",
    "    X_train = X_train.reshape(-1, num_features)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_train = X_train.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_test.shape\n",
    "    X_test = X_test.reshape(-1, num_features)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_test = X_test.reshape(num_instances, num_time_steps, num_features) \n",
    "    num_instances, num_time_steps, num_features = X_validation.shape\n",
    "    X_validation = X_validation.reshape(-1, num_features)\n",
    "    X_validation = scaler.fit_transform(X_validation)\n",
    "    \n",
    "    #reshapeing\n",
    "    X_validation = X_validation.reshape(num_instances, num_time_steps, num_features) \n",
    "    \n",
    "    # Save the scaler to a file\n",
    "    joblib.dump(scaler, './scaler/scaler.pkl')\n",
    "\n",
    "    # add an axis to nd array\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78922",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/soumen/Soumen/Dataset/KWS/data_npy\" \n",
    "class_names = ['off', 'left', 'down', 'up', 'go', 'on', 'stop', 'unknown', 'right', 'yes']  #, 'silence' , 'no'\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16  #64\n",
    "PATIENCE = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "SKIP = 1\n",
    "CLASS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c560f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train, validation and test sets\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = prepare_dataset(DATA_PATH)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import SeparableConv2D, DepthwiseConv2D, GlobalAveragePooling2D\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86362179",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Objective functions #####################\n",
    "def objective_accuracy(model, X_val, y_val):\n",
    "    \"\"\"Compute validation accuracy.\"\"\"\n",
    "    _, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def objective_model_size(model):\n",
    "    \"\"\"Compute model size in megabytes.\"\"\"\n",
    "    model_size = sum([np.prod(v.get_shape().as_list()) for v in model.trainable_weights]) #* 4 / (1024 ** 2)\n",
    "    return model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969fdbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a neighboring solution\n",
    "import random\n",
    "\n",
    "def perturb_solution(solution, bounds):\n",
    "    \"\"\"\n",
    "    Generate a perturbed solution using a concise approach within the given bounds.\n",
    "    \n",
    "    Parameters:\n",
    "        solution (list): The current solution.\n",
    "        bounds (list): Bounds for each parameter. Format:\n",
    "            - For integers: (min, max)\n",
    "            - For lists: A range for length and a range for values\n",
    "            - For categorical choices: A list of valid options\n",
    "    \n",
    "    Returns:\n",
    "        list: A new perturbed solution.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Conv layers (integer range)\n",
    "        max(bounds[0][0], min(bounds[0][1], solution[0] + random.randint(-1, 1))),\n",
    "        \n",
    "        # Filters (integer range)\n",
    "        max(bounds[1][0], min(bounds[1][1], solution[1] + random.randint(-32, 32))),\n",
    "        \n",
    "        # Kernel size (choice from list)\n",
    "        random.choice(bounds[2]),\n",
    "        \n",
    "        # FC layers (list of integers with random perturbations)\n",
    "        [max(bounds[4][0], min(bounds[4][1], fc + random.randint(-8, 8)))\n",
    "         for fc in solution[3]],\n",
    "        \n",
    "        # Batch Normalization (choice from list)\n",
    "        random.choice(bounds[5]),\n",
    "        \n",
    "        # Dropout (choice from list)\n",
    "        random.choice(bounds[6])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fa2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Depthwise Separable CNN model\n",
    "def create_dscnn_model(conv_layers, filters, kernel_size, fc_layers, use_bn, use_dropout, input_shape, depth_multiplier=1):\n",
    "\tmodel = Sequential()\n",
    "    \n",
    " # First Conv2D layer\n",
    "\tmodel.add(Conv2D(filters, kernel_size=kernel_size, strides=(1, 1), padding='same', activation='relu', input_shape=input_shape))\n",
    "\tmodel.add(Conv2D(filters, kernel_size=kernel_size, strides=(1, 1), padding='same', activation='relu'))\n",
    "\tcurrent_shape = model.output_shape\n",
    "\tif current_shape[1] > 2 and current_shape[2] > 2:\n",
    "\t\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "\t# Depthwise Separable Convolution Blocks\n",
    "\tdef depthwise_separable_block(filters):\n",
    "\t\tmodel.add(SeparableConv2D(filters, (3, 3), depth_multiplier=depth_multiplier, padding=\"same\", activation=\"relu\"))\n",
    "\t\tif use_bn:\n",
    "\t\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(SeparableConv2D(filters, (3, 3), depth_multiplier=depth_multiplier, padding=\"same\", activation=\"relu\"))\n",
    "\t\tif use_bn:\n",
    "\t\t\tmodel.add(BatchNormalization())\n",
    "\t\tcurrent_shape = model.output_shape\n",
    "\t\tif current_shape[1] > 2 and current_shape[2] > 2:\n",
    "\t\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\tif conv_layers == 1:\n",
    "\t\t\t\t\tdepthwise_separable_block(64)\n",
    "\tif conv_layers == 2:\n",
    "\t\t\t\t\tdepthwise_separable_block(128)\n",
    "\tif conv_layers == 3:\n",
    "\t\t\t\t\tdepthwise_separable_block(256)    \n",
    "\tif conv_layers == 4:\n",
    "\t\t\t\t\tdepthwise_separable_block(512)\n",
    "\n",
    "\tmodel.add(Flatten())\n",
    "\n",
    "  # Fully connected layers (dense layers)\n",
    "\tfor neurons in fc_layers:\n",
    "\t\tif neurons == 4:\n",
    "\t\t\tmodel.add(Dense(512, activation='relu'))\n",
    "\t\t\tif use_dropout:\n",
    "\t\t\t\tmodel.add(Dropout(0.5))\n",
    "\t\tif neurons == 3:\n",
    "\t\t\tmodel.add(Dense(256, activation='relu'))\n",
    "\t\t\tif use_dropout:\n",
    "\t\t\t\tmodel.add(Dropout(0.5))\n",
    "\t\tif neurons == 2:\n",
    "\t\t\tmodel.add(Dense(128, activation='relu'))\n",
    "\t\t\tif use_dropout:\n",
    "\t\t\t\tmodel.add(Dropout(0.3))\n",
    "\t\tif neurons == 1:\n",
    "\t\t\tmodel.add(Dense(64, activation='relu'))\n",
    "\t\t\tif use_dropout:\n",
    "\t\t\t\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(CLASS, activation='softmax'))\n",
    "\tmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\t# model.summary()\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Simulated Annealing with Multi-Objective Optimization #####################\n",
    "#################################################################################################\n",
    "def chaos_simulated_annealing(bounds, max_iter, cooling_rate=0.012, initial_temps=(10, 10), input_shape=input_shape, T_min=1e-6, no_improve_iter=10):\n",
    "    \n",
    "    # Initialize solution, and archive\n",
    "    current_solution = [\n",
    "        random.randint(*bounds[0]),                    # Conv layers\n",
    "        random.randint(*bounds[1]),                    # Filters\n",
    "        random.choice(bounds[2]),                      # Kernel size\n",
    "        [random.randint(*bounds[4]) for _ in range(random.randint(*bounds[3]))],  # FC layers\n",
    "        random.choice(bounds[5]),                      # Batch Normalization\n",
    "        random.choice(bounds[6])                       # Dropout\n",
    "    ]\n",
    "    temperatures = list(initial_temps)\n",
    "    archive = []\n",
    "    archive_all = []  # Initialize archive_all\n",
    "    no_improve_count = 0  # Counter for no improvement\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        print(f\"Iteration {iteration+1}/{max_iter}\")\n",
    "           \n",
    "        # Check if temperatures have reached the threshold\n",
    "        if all(t < T_min for t in temperatures):\n",
    "            print(\"Temperatures are below the minimum threshold. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Generate a neighbor solution\n",
    "        new_solution = perturb_solution(current_solution, bounds)\n",
    "\n",
    "        print(f\"current_solution: {current_solution}\")\n",
    "        print(f\"new_solution: {new_solution}\")\n",
    "        \n",
    "        # Build and evaluate models for current and new solutions\n",
    "        current_model = create_dscnn_model(*current_solution, input_shape=input_shape)\n",
    "        new_model = create_dscnn_model(*new_solution, input_shape=input_shape)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=PATIENCE)\n",
    "        current_model.fit(X_train, y_train, epochs=500, batch_size=16, callbacks=[early_stopping], verbose=0)\n",
    "        new_model.fit(X_train, y_train, epochs=500, batch_size=16, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        current_obj = [\n",
    "            objective_accuracy(current_model, X_validation, y_validation),\n",
    "            objective_model_size(current_model)\n",
    "        ]\n",
    "        new_obj = [\n",
    "            objective_accuracy(new_model, X_validation, y_validation),\n",
    "            objective_model_size(new_model)\n",
    "        ]\n",
    "        \n",
    "        print(f\"current_obj: {current_obj}\")\n",
    "        print(f\"new_obj: {new_obj}\")\n",
    "        \n",
    "        # Acceptance probability for each objective\n",
    "        accept_probs = [\n",
    "            # For accuracy (should increase)\n",
    "            1 if new_obj[0] > current_obj[0] else \n",
    "            np.exp(-(current_obj[0] - new_obj[0]) / temperatures[0]),\n",
    "            \n",
    "            # For model size (should decrease)\n",
    "            1 if new_obj[1] < current_obj[1] else \n",
    "            np.exp(-(new_obj[1] - current_obj[1]) / temperatures[1])\n",
    "        ]\n",
    "\n",
    "        # Check acceptance\n",
    "        if any(p == 1 for p in accept_probs):\n",
    "            current_solution = new_solution\n",
    "            archive_all.append((current_solution, new_obj))  # Store all evaluated good solutions\n",
    "        else:\n",
    "            all_accepted = True\n",
    "            for p in accept_probs:\n",
    "                if random.random() >= p:\n",
    "                    all_accepted = False\n",
    "                    break\n",
    "            if all_accepted:\n",
    "                current_solution = new_solution\n",
    "                archive_all.append((current_solution, new_obj))  # Store all evaluated good solutions\n",
    "\n",
    "        print(f\"accept_probs: {accept_probs}\")\n",
    "        \n",
    "        # Update temperatures using exponential cooling\n",
    "        temperatures = [t * math.exp(-cooling_rate) for t in temperatures]\n",
    "        # print(f\"Archive_all: {archive_all}\")\n",
    "        print(f\"temperatures: {temperatures}\")\n",
    "        print(\"###############################################\")\n",
    "    \n",
    "    # Moved return outside the for-loop\n",
    "    return archive, archive_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Define bounds for hyperparameters #####################\n",
    "bounds = [\n",
    "    (1, 3),                     # Number of convolutional layers\n",
    "    (16, 64),                   # Number of filters\n",
    "    [(3, 3), (5, 5)],           # Kernel size\n",
    "    (1, 3),                     # Number of fully connected layers (1 to 3 layers)\n",
    "    (128, 512),                  # Number of neurons per fully connected layer\n",
    "    [True, False],              # Batch Normalization\n",
    "    [True, False]               # Dropout\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d11c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################### Run CSA #####################\n",
    "archive_all = chaos_simulated_annealing(bounds, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ebe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Archive:\", archive_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "def store_pareto_parameter_archive_all(archive, filename=\"DS-CNN_all_archive.xlsx\"):\n",
    "    \"\"\"\n",
    "    Store the final Pareto archive in an Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "        archive (list): List of tuples where each tuple contains (solution, objectives).\n",
    "        filename (str): Name of the Excel file to save the data.\n",
    "    \"\"\"\n",
    "    # Prepare data for saving\n",
    "    data = []\n",
    "    for idx, (solution, objectives) in enumerate(archive):\n",
    "        # Flatten the solution and append objectives\n",
    "        record = {\n",
    "            \"Solution ID\": f\"Solution_{idx + 1}\",\n",
    "            \"Conv Layers\": solution[0],\n",
    "            \"Filters\": solution[1],\n",
    "            \"Kernel Size\": solution[2],\n",
    "            \"FC Layers\": \"-\".join(map(str, solution[3])),  # Join FC layers as a string\n",
    "            \"Batch Normalization\": solution[4],\n",
    "            \"Dropout\": solution[5],\n",
    "            \"accuracy\": objectives[0],\n",
    "            \"model_size\": objectives[1]\n",
    "        }\n",
    "        data.append(record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"Pareto archive saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_pareto_parameter_archive_all(archive_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transfer_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
