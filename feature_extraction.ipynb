{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e15272-8bf0-4ace-910a-583303afdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "import librosa\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa873478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set only the first GPU as visible\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        # Allow memory growth to allocate memory dynamically on the GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU configuration successful.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/home/ec.gpu/Desktop/Soumen/kws/dataset_12/\"\n",
    "save_path = \"/home/ec.gpu/Desktop/Soumen/kws/data_npy/\"\n",
    "SAMPLES_TO_CONSIDER = 16000 # 1 sec. of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hamming window function\n",
    "def hamming_window(length):\n",
    "    return 0.54 - 0.46 * np.cos(2 * np.pi * np.arange(length) / (length - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a492402",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list=['off', 'left', 'down', 'up', 'go', 'on', 'stop', 'unknown', 'no', 'right', 'yes']  #, 'silence'\n",
    "keyword_index_dict={keyword_list[i]:i for i in range(len(keyword_list))}\n",
    "print(keyword_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for STFT\n",
    "frame_length = int(SAMPLES_TO_CONSIDER * (32 / 1000))\n",
    "hop_length = int(SAMPLES_TO_CONSIDER * (16 / 1000))\n",
    "print(frame_length)\n",
    "print(hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e759986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold data\n",
    "labels = []\n",
    "log_mel_spectrograms = []\n",
    "j = 0\n",
    "\n",
    "# loop through all sub-dirs\n",
    "for i, (dirpath, dirnames, filenames) in enumerate(os.walk(DATASET_PATH)):\n",
    "\n",
    "\t# ensure we're at sub-folder level\n",
    "\tif dirpath is not DATASET_PATH:\n",
    "\n",
    "\t\t# save label (i.e., sub-folder name) in the mapping\n",
    "\t\tlabel = dirpath.split(\"/\")[-1] \n",
    "\t\tif label in keyword_list:\n",
    "\t\t\tj = j+1\n",
    "\t\t\tk = 0\t\t\n",
    "\t\t\tprint(\"\\nProcessing: '{}'\".format(label))\t\n",
    "\n",
    "\t\t\t# process all audio files in sub-dir and store MFCCs\t\n",
    "\t\t\tfor f in filenames:\n",
    "\t\t\t\tfile_path = os.path.join(dirpath, f)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tsignal, sample_rate = librosa.load(file_path)\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tprint(f\"Error loading file {file_path}: {e}\")    \n",
    "\t\t\t\tk = k+1 \n",
    "\t\t\t\tif len(signal)>=SAMPLES_TO_CONSIDER:\n",
    "\t\t\t\t\tsignal=signal[0:SAMPLES_TO_CONSIDER]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnpad = SAMPLES_TO_CONSIDER - len(signal)\n",
    "\t\t\t\t\tsignal=np.pad(signal, pad_width=npad, mode='constant', constant_values=0)[npad:]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\tsignal=signal.astype(np.float32)\t\t\t\t\t\t\t\t\n",
    "\t\t\t\tn_mels = 40  # Number of Mel bands\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\thamming_window_signal = signal * hamming_window(SAMPLES_TO_CONSIDER)  # Apply Hamming window to audio signal\n",
    "\n",
    "\t\t\t\t# Compute the log-mel-spectrogram\n",
    "\t\t\t\tmel_spectrogram = librosa.feature.melspectrogram(y=hamming_window_signal, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  sr=SAMPLES_TO_CONSIDER, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tn_mels=n_mels, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tn_fft=frame_length, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\twin_length=frame_length, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thop_length=hop_length, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcenter=False\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Convert to log scale\n",
    "\t\t\t\tlog_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\t\t\t\tf = log_mel_spectrogram\n",
    "\t\t\t\tlog_mel_spectrograms.append(log_mel_spectrogram.T)\n",
    "\t\t\t\tlabels.append(j-1)\t\t\t\t\t\n",
    "\t\t\t\tif k >999 :\n",
    "\t\t\t\t\tprint(\"{} : {} = \".format(label, j-1),k)\n",
    "\t\t\t\t\tprint(f.shape)\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\n",
    "np.save(os.path.join(save_path, 'y_log_mel_spectrograms.npy'), np.array(labels))\n",
    "np.save(os.path.join(save_path, 'X_log_mel_spectrograms.npy'), np.array(log_mel_spectrograms))\n",
    "\n",
    "print(\"Data saved in .npy format in the specified directory\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming labels and log_mel_spectrograms are your data\n",
    "labels = np.array(labels)\n",
    "log_mel_spectrograms = np.array(log_mel_spectrograms)\n",
    "\n",
    "# Split data into train (80%) and temp (20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    log_mel_spectrograms, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Split temp into validation (10%) and test (10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Save the datasets in .npy format\n",
    "np.save(os.path.join(save_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_path, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_path, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_path, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_path, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_path, 'y_test.npy'), y_test)\n",
    "print(\"Data split and saved in .npy format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
